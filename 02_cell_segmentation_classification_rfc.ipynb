{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ff8d1917-a1eb-437d-b160-59016d3c70ea",
   "metadata": {},
   "source": [
    "# Cell segmentation & classification with Random Forest Classifier (RFC)\n",
    "\n",
    "**Content**  \n",
    "1. *Cell segmentation*: Given a sparse user annotation of a single cell image, we can segment cells from the background using a pixel-wise Random Forest Classifier (RFC)  \n",
    "2. *Cell classification*: Given a separated cell image and some sparse ground truth annotation we can classify cell objects based on size and shape measurements, e.g. derived using scikit-image `regionprops`. We will do this using a RFC as well.  \n",
    "\n",
    "**Methodology**:\n",
    "- This notebook has gaps (marked with a `#TODO`) that you have to fill. \n",
    "- This notebook only contains TODOs in the section (2.) about Cell Segmentation.\n",
    "- If all gaps are filled in correctly, the notebook can be run.\n",
    "- A solution notebook with all gaps filled is provided in the GitHub repository.\n",
    "\n",
    "**Source, Inspiration**:  \n",
    "This notebook is inspired by [Pixel Classification with Random Forest](https://haesleinhuepf.github.io/BioImageAnalysisNotebooks/20a_pixel_classification/scikit_learn_random_forest_pixel_classifier.html) and [Object Classification with Random Forest](https://haesleinhuepf.github.io/BioImageAnalysisNotebooks/27_cell_classification/sklearn_object_classification.html)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aaa613d1",
   "metadata": {},
   "source": [
    "## 1. Imports and Data\n",
    "We will use the following libraries and methods:\n",
    "\n",
    "- `os`: For interacting with the operating system.\n",
    "- `RandomForestClassifier` from `sklearn.ensemble`: The implementation of the RFC by scikit-learn.\n",
    "- `io`, `filters` and `measure` from `skimage`: Read and write images, filters and other functionability.\n",
    "- `numpy` (`np`): For numerical operations.\n",
    "- `pandas` (`pd`): For tabular data manipulation and visualization.\n",
    "- `matplotlib.pyplot` (`plt`): For data visualization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03fc6513-8f08-4002-8efd-330c111f2ab1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from skimage import io, filters, measure\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69ac6411",
   "metadata": {},
   "source": [
    "We download all files that are not available (In the colab) from the internet (from the online GitHub repository of this workshop) and save them in the current workspace."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4105852a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import urllib.request  # helper library to download files\n",
    "\n",
    "if not os.path.exists('data/blobs.tif'):\n",
    "    os.makedirs('data', exist_ok=True)\n",
    "    urllib.request.urlretrieve('https://github.com/moritzmarquardt/workshop_IBEC/raw/refs/heads/main/data/blobs.tif', 'data/blobs.tif')\n",
    "    print('Loaded and saved image from remote file')\n",
    "\n",
    "if not os.path.exists('data/blobs_segmentation_annotation.npy'):\n",
    "    os.makedirs('data', exist_ok=True)\n",
    "    urllib.request.urlretrieve('https://github.com/moritzmarquardt/workshop_IBEC/raw/refs/heads/main/data/blobs_segmentation_annotation.npy', 'data/blobs_segmentation_annotation.npy') \n",
    "    print('Loaded and saved annotations from remote file')\n",
    "\n",
    "if not os.path.exists('data/blobs_classification_annotation.npy'):\n",
    "    os.makedirs('data', exist_ok=True)\n",
    "    urllib.request.urlretrieve('https://github.com/moritzmarquardt/workshop_IBEC/raw/refs/heads/main/data/blobs_classification_annotation.npy', 'data/blobs_classification_annotation.npy') \n",
    "    print('Loaded and saved class annotations from remote file')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f69d1d14",
   "metadata": {},
   "outputs": [],
   "source": [
    "image = io.imread('data/blobs.tif')\n",
    "\n",
    "plt.figure(figsize=(4, 4))\n",
    "plt.imshow(image, cmap='gray')\n",
    "plt.colorbar()\n",
    "plt.title('Original example image')\n",
    "plt.show()\n",
    "\n",
    "print(f'Image shape: {image.shape}')\n",
    "print(f'Image type: {image.dtype}')\n",
    "print(f'Image max value: {image.max()}')\n",
    "print(f'Image min value: {image.min()}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c61566b",
   "metadata": {},
   "source": [
    "## 2. Cell segmentation using RFC"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03d61166",
   "metadata": {},
   "source": [
    "### 2.1 (optional) Interactive manual annotation\n",
    "**Important**: This is not possible in a Google Colab but only on a local python runtime. Therefore it is skipped in this tutorial, but the interested reader can try this out.\n",
    "\n",
    "Using the two cells below, it is possible to annotate the image manually and save the annotations to a file to use them in the following.  \n",
    "Feel free to test this out if you run this notebook on your PC."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee0657c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import napari\n",
    "\n",
    "# # start napari\n",
    "# viewer = napari.Viewer()\n",
    "\n",
    "# # add image\n",
    "# viewer.add_image(image)\n",
    "\n",
    "# # add an empty labels layer and keep it in a variable\n",
    "# labels = viewer.add_labels(np.zeros(image.shape).astype(int))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1ebf40b",
   "metadata": {},
   "source": [
    "After running the cell above and annotating manually in the napari window using the brush tool, execute the cell below to save the annotation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1078cb64",
   "metadata": {},
   "outputs": [],
   "source": [
    "# annotations = labels.data\n",
    "\n",
    "# np.save('data/blobs_manual_annotations.npy', annotations)\n",
    "\n",
    "# napari.utils.nbscreenshot(viewer)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34da815b",
   "metadata": {},
   "source": [
    "### 2.2 Load and visualize annotations\n",
    "\n",
    "Here we load the annotations that were saved in the part above or the presaved example annotation is used.  \n",
    "Annotations are the following:\n",
    "- value 1 is background\n",
    "- value 2 is a cell"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f17ffe4",
   "metadata": {},
   "outputs": [],
   "source": [
    "annotations = np.load('data/blobs_segmentation_annotation.npy')\n",
    "\n",
    "# visualize the manual annotations\n",
    "fig, ax = plt.subplots(1, 2, figsize=(8, 4))\n",
    "ax = ax.ravel()\n",
    "im0 = ax[0].imshow(annotations)\n",
    "ax[0].set_title('Manual Annotations')\n",
    "cbar = plt.colorbar(im0, ax=ax[0], orientation='vertical')\n",
    "cbar.set_ticks([0, 1, 2])\n",
    "cbar.set_ticklabels([0, 1, 2])\n",
    "\n",
    "im1 = ax[1].imshow(image, cmap='gray')\n",
    "ax[1].imshow(annotations, alpha=0.5)\n",
    "ax[1].set_title('Overlay of Image and Annotations')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6df48d3e",
   "metadata": {},
   "source": [
    "### 2.3 Feature extraction\n",
    "In this subsection, we generate features for each pixel from the original image since we will be classifying pixels in the end.\n",
    "The idea is to have different values (features) for each pixel to make the classification stronger.  \n",
    "These values will be obtained by applying different filters to the image.\n",
    "\n",
    "**TODO**:\n",
    "1. Implement the two filters from skimage.filters to generate features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a17c1ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# define additional features\n",
    "\n",
    "#TODO: Use the filters.gaussian function from the skimage module with sigma=2\n",
    "blurred_image = \n",
    "\n",
    "#TODO Use the filters.sobel function from the skimage module\n",
    "edges_image = \n",
    "\n",
    "# The ravel() function turns a nD image into a 1-D image.\n",
    "features = [\n",
    "    image.ravel(),\n",
    "    blurred_image.ravel(),\n",
    "    edges_image.ravel()\n",
    "]\n",
    "\n",
    "# Transpose the features to have the shape (n_samples, n_features)\n",
    "features = np.asarray(features).T\n",
    "\n",
    "# visualize the features as a pandas dataframe\n",
    "df_features = pd.DataFrame(features)\n",
    "df_features.columns = ['original', 'blurred', 'edges']\n",
    "df_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8b28d82",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# visualise feature images\n",
    "fig, axes = plt.subplots(1, 3, figsize=(12,4))\n",
    "\n",
    "# reshape(image.shape) is the opposite of ravel() here. We just need it for visualization.\n",
    "axes[0].imshow(df_features[\"original\"].values.reshape(image.shape), cmap='gray')\n",
    "axes[0].set_title('Original')\n",
    "axes[1].imshow(df_features[\"blurred\"].values.reshape(image.shape), cmap='gray')\n",
    "axes[1].set_title('Blurred using Gaussian filter')\n",
    "axes[2].imshow(df_features[\"edges\"].values.reshape(image.shape), cmap='gray')\n",
    "axes[2].set_title('Edges using Sobel filter')\n",
    "plt.show()\n",
    "\n",
    "#TODO (for fast people) Play around with the sigma parameter of the gaussian blur filter. A larger sigma corresponds to a larger gaussian kernel and thus more blurring. Find a balance so that noise is reduced but edges are still visible."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a34c89d0",
   "metadata": {},
   "source": [
    "### 2.4 Format training data\n",
    "The RandomForestClassifier expects an X array of shape (n_samples, n_features) and a y array of shape (n_features)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "083a2d90",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = features\n",
    "y_train = annotations.ravel()\n",
    "\n",
    "# use only annotated pixels for training\n",
    "mask = y_train > 0\n",
    "X_train = X_train[mask]\n",
    "y_train = y_train[mask]\n",
    "\n",
    "print(\"X_train shape\", X_train.shape)\n",
    "print(\"y_train shape\", y_train.shape)\n",
    "\n",
    "# visualize the training data\n",
    "df_train_x_y = pd.DataFrame(X_train)\n",
    "df_train_x_y['label'] = y_train\n",
    "# rename columns to feature_i\n",
    "df_train_x_y.columns = [f'feature_{i}' for i in range(3)] + ['label']\n",
    "df_train_x_y.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc2f5c06",
   "metadata": {},
   "source": [
    "### 2.5 Train the Random Forest Classifier\n",
    "\n",
    "**TODO**:\n",
    "1. Use the scikit-learn's RandomForestClassifier to create and fit the Classifier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4a1b31a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# train test split using df_train_x_y\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train_split, X_val, y_train_split, y_val = train_test_split(X_train, y_train, test_size=0.25, random_state=0) \n",
    "print(X_train_split.shape, y_train_split.shape)\n",
    "print(X_val.shape, y_val.shape)\n",
    "\n",
    "\n",
    "#TODO: initialize a RandomForestClassifier with a max_depth of 2 and a random_state of 0. \n",
    "classifier = \n",
    "\n",
    "#TODO: Train the classifier with the training data X_train_split and y_train_split (use the fit method of the classifier)\n",
    "\n",
    "\n",
    "#evaluate the classifier\n",
    "print(f'Validation accuracy: {classifier.score(X_val, y_val):.2f}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55259d7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# visualise one of the trees of the \"random forest\"\n",
    "from sklearn.tree import plot_tree\n",
    "plt.figure(figsize=(6, 4))\n",
    "plot_tree(classifier.estimators_[0], filled=True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6577131",
   "metadata": {},
   "source": [
    "### 2.6 Use the classifier for segmentation\n",
    "\n",
    "**TODO**:\n",
    "1. Use the prediciton method of the classifier to predict all pixels of the image and get a segmented image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98076298",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_predict = features # use all features for all pixels to predict\n",
    "\n",
    "#TODO Use the predict function of the classifier to predict the annotations (cell or background) for the prediction data X_predict\n",
    "res = \n",
    "\n",
    "res = res - 1 # we subtract 1 to make background = 0 (this is necessary, since the background is annotated with 1 in the manual annotations)\n",
    "\n",
    "# visualize the results\n",
    "res_image = res.reshape(image.shape) # reshape the 1D result array back to the 2D image shape\n",
    "fig, ax = plt.subplots(1, 3, figsize=(12, 4))\n",
    "ax = ax.ravel()\n",
    "ax[0].imshow(image, cmap='gray')\n",
    "ax[0].set_title('Original image')\n",
    "\n",
    "# Display predicted segmentation without specifying cmap (default)\n",
    "segmentation_plot = ax[1].imshow(res_image)\n",
    "ax[1].set_title('Predicted segmentation')\n",
    "\n",
    "# Extract colormap colors\n",
    "cmap = segmentation_plot.get_cmap()\n",
    "norm = segmentation_plot.norm  # Normalization of values (0 to 1)\n",
    "background_color = cmap(norm(0))  # Color for background (0)\n",
    "segment_color = cmap(norm(1))  # Color for segment (1)\n",
    "\n",
    "# Add a legend with the correct colors\n",
    "legend_labels = ['Background (0)', 'Segmented cell (1)']\n",
    "colors = [background_color, segment_color]  # Extracted colors\n",
    "patches = [plt.Line2D([0], [0], color=color, marker='o', linestyle='None', markersize=10, label=label)\n",
    "           for color, label in zip(colors, legend_labels)]\n",
    "ax[1].legend(handles=patches, loc='upper right')\n",
    "\n",
    "ax[2].imshow(image, cmap='gray', alpha=0.8)\n",
    "ax[2].contour(res_image == 1, colors='r')\n",
    "ax[2].set_title('Overlay of image and segment contours')\n",
    "## add legend for contours\n",
    "patches = [plt.Line2D([0], [0], color='r', marker='o', linestyle='None', markersize=10, label='Segment contours')]\n",
    "ax[2].legend(handles=patches, loc='upper right')\n",
    "\n",
    "plt.show()\n",
    "\n",
    "#TODO (for fast people) Play around with the max_depth parameter to see how it affects the classification performance. A larger max_depth corresponds to a more complex model that can capture more details in the data. However, it can also lead to overfitting. Find a balance that works well for this data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61860efd",
   "metadata": {},
   "source": [
    "### 2.7 Labeling the segmented objects\n",
    "In this step, each connected component in the segmented image is assigned a unique label. This process is essential for the next phase, where we classify individual objects based on their properties.\n",
    "\n",
    "**TODO**: \n",
    "1. label the segmented objects"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5055a665",
   "metadata": {},
   "outputs": [],
   "source": [
    "#TODO Use the measure.label function from the skimage module to label the objects in the segmentation res_image\n",
    "labels = \n",
    "\n",
    "plt.figure(figsize=(4, 4))\n",
    "plt.imshow(labels)\n",
    "cbar = plt.colorbar()\n",
    "plt.title('Labeled objects')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9af6b3f6",
   "metadata": {},
   "source": [
    "### Compare with non ml methods\n",
    "\n",
    "- Otsu thresholding  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afdf0457",
   "metadata": {},
   "outputs": [],
   "source": [
    "# otsu thresholding\n",
    "thresh = filters.threshold_otsu(image)\n",
    "binary = image > thresh\n",
    "fig, ax = plt.subplots(1, 3, figsize=(10, 10))\n",
    "ax[0].imshow(image, cmap='gray')\n",
    "ax[0].set_title('Original image')\n",
    "ax[1].imshow(res_image)\n",
    "ax[1].set_title('RFC Segmentation')\n",
    "ax[2].imshow(binary)\n",
    "ax[2].set_title('Otsu thresholding')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "776dbefc",
   "metadata": {},
   "source": [
    "Otsu thresholding works well here since the example image is greyscale, but these classical methods get to their limits when images are colourful and more complex."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9733d96",
   "metadata": {},
   "source": [
    "## 3. Classification using RFC\n",
    "\n",
    "Now we will train a second Random Forest classifier (RFC) to classify each segmented and labeled object based on its shape properties.\n",
    "\n",
    "\n",
    "### 3.1 Load annotations\n",
    "Similar to the segmentation, we need an annotation of the image which can be done manually by using for example the napari plugin similar to the segmentation part. Here we load an annotation that has already been done.  \n",
    "\n",
    "Annotations are the following:\n",
    "- value 1: large cells\n",
    "- value 2: elongated cells\n",
    "- value 3: small cells"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9aa813f5-a48a-46f1-b42f-8ee5b45e8f8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# annotation with classes:\n",
    "# 1 (blue): large cells\n",
    "# 2 (green): elongated cells\n",
    "# 3 (yellow): small cells\n",
    "class_annotation = np.load('data/blobs_classification_annotation.npy')\n",
    "\n",
    "# visualize\n",
    "fig, ax = plt.subplots(1,3, figsize=(15,15))\n",
    "ax[0].imshow(image, cmap='gray')\n",
    "ax[0].set_title('Original image')\n",
    "ax[1].imshow(labels)\n",
    "ax[1].set_title('RFC Segmentation')\n",
    "# Display the annotation image with transparency\n",
    "ax[2].imshow(image, cmap='gray')\n",
    "annotation_plot = ax[2].imshow(class_annotation, alpha=0.8)\n",
    "ax[2].set_title('Manual annotations')\n",
    "\n",
    "# Extract colormap and normalization from the annotation plot\n",
    "cmap = annotation_plot.get_cmap()\n",
    "norm = annotation_plot.norm\n",
    "\n",
    "# Define the colors and labels for the legend\n",
    "legend_labels = ['Large cells (1)', 'Elongated cells (2)', 'Small cells (3)']\n",
    "colors = [cmap(norm(1)), cmap(norm(2)), cmap(norm(3))]\n",
    "\n",
    "# Create legend patches\n",
    "patches = [plt.Line2D([0], [0], color=color, marker='o', linestyle='None', markersize=10, label=label)\n",
    "           for color, label in zip(colors, legend_labels)]\n",
    "\n",
    "# Add legend to the lower left corner\n",
    "ax[2].legend(handles=patches, loc='lower left')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "543aa2f9-cbe3-4eb7-b367-74e3ec18edd2",
   "metadata": {},
   "source": [
    "### 3.2 Feature extraction\n",
    "The first step to classify objects according to their properties is feature extraction. This is analog to building the feature stack in the segmentation part. The difference here is, we do not extract features for each pixel, but for each segmented object (connected labeled pixels) since we will be classifying the objects in the end."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49cbb9fd-74e9-4c91-b4d7-12bd7a9f4c4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "stats = measure.regionprops(labels, intensity_image=image)\n",
    "\n",
    "# read out specific measurements as features\n",
    "label_ids =          np.asarray([s.label for s in stats])\n",
    "areas =              np.asarray([s.area for s in stats])\n",
    "minor_axis_lengths = np.asarray([s.minor_axis_length for s in stats])\n",
    "major_axis_lengths = np.asarray([s.major_axis_length for s in stats])\n",
    "\n",
    "# compute additional parameters\n",
    "aspect_ratios = major_axis_lengths / minor_axis_lengths"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01c007cd-2de4-4f2c-8089-517b9cd6aef0",
   "metadata": {},
   "source": [
    "We also read out the maximum intensity of every labeled object from the ground truth annotation. These values will serve to train the classifier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8464589c-685d-48f5-951e-d89227cd3775",
   "metadata": {},
   "outputs": [],
   "source": [
    "annotation_stats = measure.regionprops(labels, intensity_image=class_annotation)\n",
    "\n",
    "annotated_class = np.asarray([s.intensity_max for s in annotation_stats])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b34a5fee-b050-4a3e-aa85-ce4d6997d923",
   "metadata": {},
   "source": [
    "### 3.3 Format data\n",
    "\n",
    "To look at the data before it is fed to the training, we visualize it as [pandas DataFrame](https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.html). Note: The rows with `annotated_class=0` correspond to labels that have not been annotated."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4572be4-6f29-4f32-bdeb-78f395b0ef6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = {\n",
    "    'label': label_ids,\n",
    "    'area': areas,\n",
    "    'minor_axis': minor_axis_lengths,\n",
    "    'major_axis': major_axis_lengths,\n",
    "    'aspect_ratio': aspect_ratios,\n",
    "    'annotated_class': annotated_class\n",
    "}\n",
    "\n",
    "table = pd.DataFrame(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3234d247-c44c-456c-ba1f-bd88060a960a",
   "metadata": {},
   "source": [
    "From that table, we extract now a table that only contains the annotated rows/labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a01d040-d03c-44ee-8e18-c862cf8acfc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "annotated_table = table[table['annotated_class'] > 0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cdcacd2d-d623-4bab-959b-7958d86b2a72",
   "metadata": {},
   "source": [
    "As we do not want to use all columns for training, we now select the right columns. It is recommended to write a short convenience function `select_data` for this, because we will reuse it later for prediction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93adf92f-6f35-4538-9f9f-1748eb1767d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def select_data(table):\n",
    "    return np.asarray([\n",
    "        table['area'],\n",
    "        table['aspect_ratio']\n",
    "    ])\n",
    "\n",
    "training_data = select_data(annotated_table).T\n",
    "ground_truth = annotated_table['annotated_class'].tolist()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b19e418-6ff4-4c70-b50e-cd565c99921e",
   "metadata": {},
   "source": [
    "### 3.4 Train the Random Forest classifier\n",
    "\n",
    "Next, we can train the [Random Forest Classifer](https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestClassifier.html). It needs training data and ground truth in the format presented above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d844764a-c440-4487-a95b-081dc5676e59",
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier_cells = RandomForestClassifier(max_depth=2, n_estimators=10, random_state=0)\n",
    "classifier_cells.fit(training_data, ground_truth)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c806ad1-87b1-4588-9bc7-fb4c2901be2d",
   "metadata": {},
   "source": [
    "### 3.5 Predict the ojects classes\n",
    "\n",
    "To apply a classifier to the whole dataset, or any other dataset, we need to bring the data into the same format as used for training. We can reuse the function `select_data` for that. Furthermore, we need to drop rows from our table where not-a-number values appeared."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28b3ab98-c534-4500-a74d-519ada717875",
   "metadata": {},
   "outputs": [],
   "source": [
    "table_without_nans = table.dropna(how=\"any\")\n",
    "\n",
    "all_data = select_data(table_without_nans).T\n",
    "all_data\n",
    "# where is alldata inf\n",
    "np.where(np.isinf(all_data))\n",
    "all_data[all_data == np.inf] = np.nan"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aca54770-4321-48e2-bffe-0a6935d18a27",
   "metadata": {},
   "source": [
    "We can then hand over `all_data` to the classifier for prediction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a42cad7-5fd2-4ab0-bfa1-fa9ad5ff544e",
   "metadata": {},
   "outputs": [],
   "source": [
    "table_without_nans['predicted_class'] = classifier_cells.predict(all_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b38c5243-fccf-4a7d-9bba-8aabf01c9e66",
   "metadata": {},
   "source": [
    "We can then merge the table containing the `predicted_class` column with the original table. In the resulting `table_with_prediction`, we still need to decide how to handle `NaN` values. It is not possible to classify those because measurements are missing. Thus, we replace the class of those with 0 using [`fillna`](https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.fillna.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d25ea3e4-2108-4892-96ff-fbdc3e382bd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# merge prediction with original table\n",
    "table_with_prediction = table.merge(table_without_nans, how='outer', on='label')\n",
    "# replace not predicted (NaN) with 0\n",
    "table_with_prediction['predicted_class'] = table_with_prediction['predicted_class'].fillna(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3a7b91b-723d-47ef-b1da-fd6e454407c8",
   "metadata": {},
   "source": [
    "From that table, we can extract the column containing the prediction and use [`replace_intensities`](https://clij.github.io/clij2-docs/reference_replaceIntensities) to generate a `class_image`. The background and objects with NaNs in measurements will have value 0 in that image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b91f8dd3-1cc5-40cb-9305-c83102c6f228",
   "metadata": {},
   "outputs": [],
   "source": [
    "# we add a 0 for the class of background at the beginning\n",
    "predicted_class = [0] + table_with_prediction['predicted_class'].tolist() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a2d58d7-30ae-455c-8dc1-b4fba590b2f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#class_image = replace_intensities(labels, predicted_class)\n",
    "class_image = labels.copy()\n",
    "for s in stats:\n",
    "    class_image[class_image == s.label] = predicted_class[s.label]\n",
    "\n",
    "plt.imshow(class_image)\n",
    "cbar = plt.colorbar()\n",
    "cbar.set_ticks([0, 1, 2, 3])\n",
    "cbar.set_ticklabels([0, 1, 2, 3])\n",
    "plt.title('Predicted classes')\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
